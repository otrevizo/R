---
title: 'Normal Distribution Vignette: WORK IN PROGRESS'
author: "Oscar A. Trevizo"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# WORK IN PROGRESS

Use y <- rnorm(100) to generate a random sample of size 100 from a normal distribution. 

```{r,eval=TRUE}
#R Code here

# Lets assign the sample size of 100 to variable "S" to reuse it in this assignment.
S <- 100

# Now get S number of Normally distributed random numbers.
y <- rnorm(S)

```

### 1A

Calculate the mean and standard deviation of y.
```{r, eval=TRUE}
#R code here

# Mean and Std. Deviation funcitons:
mu <- mean(y)
sigma <- sd(y)

# Let's print them using the paste() function.
# Keep the display to 2 decimal points.
paste(' Mean of y = ', round(mu, digits=2))
paste(' Standard deviation of y = ', round(sigma, digits=2))

```

- The _rnorm(100)_ function generates $100$ Normally distributed random numbers with $mean=0$ and standard deviation $sd=1$. 
- While the theoretical _mean_ and _standard deviation_ in _rnorm()_ are $0$ and $1$ respectively, 
the experiments (each try or trial) do not result in those exact values due to randomness.
- Therefore, the resulting _mean_ is close to $0$, and the resulting _standard deviation_ is close to $1$, but not necessarily exactly $0$ and $1$ respectively due to randomness.
- The  _paste()_ function printed two results of the _mean_ and _standard deviation_.
- Note: I will be using the notation _sigma_ to refer to the standard deviation in much of my code.

Use a loop to repeat the above calculation 30 times. Store the 30 means in a vector named AVG. Calculate 
the standard deviation of the values in AVG.

```{r, eval=TRUE}
#R code here

# Let's assign our 30 experiments to a variable "N".
N <- 30

# Initialize vector AVG of size 30
AVG <- numeric(N)

# Run a loop that gets a new data sample, calculates the mean and sd, and stores the result in AVG.
for (i in 1:N) {
  # AVG is an indexed vector that will take values from A[1] to A[30] as we loop through.
  # Store the mean of a Normally distributed sample of size S into AVG[i]. It will happen N times.
  AVG[i] <- mean(rnorm(S))
}

# Coming out of the loop, we have a vector AVG with 30 values.
# Calculate the standard deviation of AVG.
SIGMA <- sd(AVG)

# Display SD below
paste('The standard deviation of AGG is ', round(SIGMA, 2))
```

- _AVG_ is a vector with $30$ values. Each value in the vector is the _mean_ of $100$ Normally distributed random numbers generated by _rnorm()_.
- _SIGMA_ is a numeric variable that contains the _standard deviation_ of those $30$ values from vector _AVG_.
- The result, the _standard deviation_ of the _means_ from those $30$ _means_ (the experiments or trials), is displayed above by the code.
- Notice how _SIGMA_ is much smaller than the _standard deviation_ obtained in the previous question where we were calculating the sigma of $100$ Normally distributed random numbers.
- This time, _SIGMA_ is calculated on the _means_, so it results in a smaller number, more narrow value.
- The reason _SIGMA_ is so small is, the _means_ tend to be close to $0$, over and over. We are no longer measuring the _standard deviation_ on the $100$ random numbers, but on the _mean_ of those numbers.


Run (b) 4 times, showing each of the distributions of 30 means in a normal probability plot and box plot.

```{r, fig.height=12, fig.width=8, eval=TRUE}
#R code here

par(mfrow = c(4, 2))

# Loop through the trial runs, or experiments.
for (i in 1:4) {
  # Run a loop that gets a new data sample, calculates the mean and sd, and stores the result in AVG.
  # First get an N vector AVG with the means of random numbers
  for (j in 1:N) {
    # AVG is an indexed vector that will take values from A[1] to A[30] as we loop through.
    # Store the mean of a Normally distributed sample of size S into AVG[i]. It will happen N times.
    AVG[j] <- mean(rnorm(S))
  }
  SHAPIRO <- shapiro.test(AVG)
  qqnorm(AVG, 
         ylim=c(-0.2, 0.2), 
         col='blue', 
         main=paste('Q-Q Plot. Normality p-value=', round(SHAPIRO$p.value, 2)))
  qqline(AVG, 
         ylim=c(-0.2, 0.2), 
         col='red')
  boxplot(AVG, 
          col='lightblue',
          main=paste('Boxplot. Sample size ', N),
          xlab=paste('Trial ', i))
}


```

- **Preliminary comments:**
- The results provide $4$ sets of $2$ plots each: A _Normal Probability Plot (a.k.a. Q-Q Plot)_ and a _boxplot_ for each of the $4$ experiments.
- I added a straight diagonal line to the _Q-Q plot_ to help us visualize the results.
- In addition, I included _Shapiro-Wilk Tests_ to test the $H_o$ _null hypothesis_ for each experiment run.
- The _null hypothesis_ $H_o$ of a _Shapiro-Wilk Test_ states that the sample was generated from a _Normal distribution_. The $H_A$ _alternate hypothesis_ rejects the _null hypothesis_ stating that the same did not come from a _Normal distribution_.
- We want to know if we should _reject_ the _null hypothesis_, or keep the _null hypothesis_. 
- The _null hypothesis_ test has a key parameter called the _p-value_.
- If the _p-value_ is smaller than $0.05$ then we will reject the $H_o$ _null hypothesis_ and will propose that our sample does not come from a _Normal distribution_.
- And if the _p-value_ is greater than $0.05$, then we will keep the $H_o$ _null_hypothesis_, stating that the _null hypothesis_ is possible, and we propose the sample comes from a _Normally distribution_.
- If the distribution were to be Normal, the _Q-Q plot_ would have the data points ( _the dots_ ) aligned close to the straight diagonal line.
- We would also want to see _Shapiro-Wilk Test_ _p-value_ smaller than $0.05$.

- **Results:**
- We do see a tendency of the dots following the _Q-Q line_, even though the dots are not exactly on top of the line in all four experiments. 
- In fact, some experiments exhibit _tails_ either at the start or at the end of the _Q-Q plot_. So we want to know more about the distribution.
- We will not get _all_ of the points lined up directly on top pf the _Q-Q line_ because there is always some noise or unexplained variation in the observations (hence being a _random_ sample).
- The _Shapiro-Wilk Test_ _p-values_, included on the title of each _Q-Q plot_, are all greater than $0.05$. Therefore, we will keep the $H_o$ _null hypothesis_ and state that the samples came from a _Normal distribution_.
- The _boxplots_ provide another visual to help us assess or form an opinion on whether the data may follow a _Normal_ distribution or not.
- The _boxplots_ are very consistent. The range between 1st and 3rd quartiles is _narrow_ based on an observation: The _boxplots_ have those quartiles between approximately $-0.1$ and $0.1$ in all cases, sometimes even closer.
- Therefore, the _boxplots_ support the notion that these sampes came from _Normal distributions_. 
- In clonclusion, these sample came from _Normal distributions_. 


## Part 2 (30 points)

Use mfrow to set up the layout for a 3 by 4 array of plots. In the top 4 panels, show normal probability plots for 4 separate “random” samples of size 10, all from a normal distribution. In the middle 4 panels, display plots for samples of size 100. In the bottom 4 panels, display plots for samples of size 1000. How does the appearance of plots change as the sample size changes?  

```{r, fig.height=8, fig.width=12, eval=TRUE}
#R Code Here

par(mfrow = c(3, 4))

# S is a vector that contains hte different sample sizes 10, 100, and 1000.
S <- c(10, 100, 1000)

# Now we loop through the 3 sets of 4 experiments each.
for (i in 1:3) {
  # Now run the 4 experiments
  for (j in 1:4) {
    # Run a trial sample size S[i] based on the S vector above.
    SAMPLE <- rnorm(S[i])
    SHAPIRO <- shapiro.test(SAMPLE)
    # ntest.p.value[i] <- SHAPIRO$p.value
    qqnorm(SAMPLE, 
          ylim=c(-2, 2), 
          col='blue', 
          main=paste('Q-Q Plot. Normality p-value=', round(SHAPIRO$p.value, 2)))
    qqline(SAMPLE, 
          ylim=c(-2, 2), 
          col='red')
  }
}

```

- What do you notice happens as the sample increases. It resembles a straight line.
- In all cases, _Shapiro-Wilk Tests_ indicates that we should _keep_ the _null Hypotheis_ that our sample comes from a _Normal distribution_ and it does. But the degree in which it proposes such _null hypothesis_, based on the _p-value_ varies as we increase the _sample size_.
- WE can see the impact from the _Central Limit Theorem (CLT)_. As we increase the _sample size_ we get values that get closer to a _Normal distribution_.
- The top panel of $4$ plots with _sample size_ of $10$ exhibits irregularity. We can see that in how the _data points_ follow (or not follow) the _Q-Q line_. The _Q-Q line_ is also irregular (notice the difference in slopes).
- The middle panel of $4$ plots with _sample size_ of $100$ is an improvement over the first panel. But it still exhibits irregularities. We tend to have _tails_ in either end of the plot. 
- The bottom panel of $4$ plots with _sample size_ of $1000$ offers a very evident _Normal distribution_. We can be highly confient that the samples came from a _Normal distribution_, without a question.


## Part 3 (30 points)  

The statement x <- rchisq(n, 1) generates n random values from a chi-squared distribution with one degree of freedom. The statement x <- rt(n, 1) generates n random values from a t-distribution with one degree of freedom. Make normal probability plots using 30 random values for various degrees of freedom from each of these distributions. Approximately how large degrees of freedom is necessary, in each instance, to obtain a consistent normal distribution shape?

```{r, fig.height=12, fig.width=8, eval=TRUE}
#R Code Here

# Sample size
N <- 30

# Let's bound our story to a maximum degrees of freedom
Max_D <- N-1

TRIALS <- 100
# Initialize two dataframes: one for chisq and one for t distribution
chi_p.value <- as.data.frame(matrix(NA, nrow = TRIALS*Max_D, ncol=3))
t_p.value <- as.data.frame(matrix(NA, nrow = TRIALS*Max_D, ncol=3))

par(mfrow = c(5, 4))

# Initialize observation number
obs <- 0

# Loop from 1 to S-1 degree of freedom experiments each.
# for (i in 1:(N-1)) {
for (i in 1:Max_D) {

  # Now I am going to run 100 trials for each experiment
  # To do multiple tests and plot a boxplot of p-values
  # Trials within an experiment
  for (j in 1:TRIALS) {
    
    obs <- obs + 1
    # Run Chi-Square distribution trials
    CHI_SAMPLE <- rchisq(N, i)
    CHI_SHAPIRO <- shapiro.test(CHI_SAMPLE)
    
    # Store it in the dataframe
    chi_p.value[obs, 1] <- i
    chi_p.value[obs, 2] <- CHI_SHAPIRO$p.value
    chi_p.value[obs, 3] <- 'chisq'
    
    # Run t-distribution trials.
    T_SAMPLE <- rt(N, i)
    T_SHAPIRO <- shapiro.test(T_SAMPLE)
    
    # Store it in the dataframe
    t_p.value[obs, 1] <- i
    t_p.value[obs, 2] <- T_SHAPIRO$p.value
    t_p.value[obs, 3] <- 't'
    
  }
  
  # Q-Q plot the last trial
  qqnorm(CHI_SAMPLE,
        #ylim=c(-2, 2),
        col='blue',
        main=paste('Q-Q Chi: DF', i, 'p-val ', round(CHI_SHAPIRO$p.value,2)),
        cex.main=1,)
  qqline(CHI_SAMPLE,
        #ylim=c(-2, 2),
        col='red')

  qqnorm(T_SAMPLE,
        #ylim=c(-2, 2),
        col='green',
        main=paste('Q-Q T: DF', i, 'p-val ', round(T_SHAPIRO$p.value,2)),
        cex.main=1,)
  qqline(T_SAMPLE,
        #ylim=c(-2, 2),
        col='red')

}
```

#### Use boxplot to visualize DF impact


```{r, fig.height=4, fig.width=8, eval=TRUE}
# BOxplot of p-values

#chi_p.value[complete.cases(chi_p.value,)]
colnames(chi_p.value) <- c('deg_of_freedom', 'p_value', 'distribution')
colnames(t_p.value) <- c('deg_of_freedom', 'p_value', 'distribution')

par(mfrow = c(1, 2))

boxplot(p_value~deg_of_freedom, chi_p.value, 
        col = 'lightblue',
        main='ChiSq: deg. freedom vs. p-values',
        ylab = 'Normality p-value (red line at 0.05)',
        xlab = 'ChiSq distribution degrees of freedom',
        cex.main=1,
        cex.axis=0.7)
abline(v=20, h=0.05, col='red')

boxplot(p_value~deg_of_freedom, t_p.value, 
        col = 'lightgreen',
        main = 't: deg. freedom vs. p-values',
        ylab = 'Normality p-value (red line at 0.05)',
        xlab = 'T distribution degrees of freedom',
        cex.main=1,
        cex.axis=0.7)
abline(v=6, h=0.05, col='red')

```

- In both cases, _ChiSquare distribution and t-distribtution_, increasing the _number_ of _degrees of freedom_ makes the drawn distribution closer to a _Normally distributed_ distribution.
- The sample size is fixed at $30$ as stated in the problem.
- We can visually perceive _Q-Q plots_ with data points aligning closer to the diagonal as we increase the _number_ of _degrees of freedom_.
- Evidently, these are random samples. And as such, the random sample will varie from experiment to experiment.
- Therefore, I calculated the _Shapiro-Wilk Tests_ for multiple experiment to assess an acceptable value of _degrees of freedom_ for each distribution case.
- I store the _p-values_ of the _Shapiro-Wilk Tests_ in a _dataframe_. I ran $100$ trials for each _degree of freedom_ and for wach type of distribution.
- The $H_o$ _null hypothesis_ states that the sample likely came from a _Normal_ distribution. If the _p-value_ is higher than $0.05$ we keep the _null hypothesis_ stating that the same is _Normally distributed_.
- The$H_A$ _alternative hypothesis_ states the opporsite, the sample des not come from a _Normal_ distribution, and we reject and _null hypothesis_.
- Then I created _boxplots_ for each type of distribution, _ChiSquare and t distribution_.
- The chart shows a _boxplot_ for each _degrees of freedom_ value.
- We can observe an upward or positive trend as we increase the _degrees of freedom_, the _p-value_ increases.
- We also observe that the upward trend is more pronounced for the _t-distribution_ than for the _ChiSquare distribution_.
- Furthermore, I included a _red_ line on the key $0.05$ _p-value_ mark.
- And I included a vertical _red_ line where the _interquartile range_ of the _boxplots_ clear, and are above the $0.05$ _p-value_ mark.
- For a _ChiSquare distribution_, when we have _degrees of freedom_ greater than $20$ we have a  _p-values_ above the $0.05$ for the _interquartile range_ (the boxes from the boxplot).
- For a _t distribution_, when we have _degrees of freedom_ at least $6$ we have a  _p-values_ above the $0.05$ for the _interquartile range_ (the boxes from the boxplot).
- Therefore, to answer the question, for $30$ observations: _'Approximately how large degrees of freedom is necessary, in each instance, to obtain a consistent normal distribution shape?_
- The answer depends on how strict is the use case, the impact to people.
- For $ChiSquare$, I would use at least $20$ _degrees of freedom_ to have consistent draws of _Normally distributed_ data.
- For $t-distribution$, I would use at least $6$ _degrees of freedom_ to have consistent draws of _Normally distributed_ data.



## Part 4 (10 points)  

A bank has kept records of the checking balances of its customers and determined that the average daily balance of its customers is \$350 with a standard deviation of \$58. A random sample of 144 checking accounts is selected.


### 4A

What is the probability that the sample mean will be less than $356.60?

```{r, fig.height=8, fig.width=10, eval=TRUE}
#R Code Here

par(mfrow = c(2, 2))

# The question clearly focuses on the mean of a sample with size 144 accounts.

# Before I go there, I would like to display the statistics on the probability of a 
# single draw, that is drawing a single account from the full population accounts in this bank.
#
# So fo from 350-250 = 100 to 350+250=600 and make it a 1 dollar increment, so get 500 points.

# This will be our horizontal axis in our density plot of the accounts balance.
balance <- pretty(c(100, 600), 500)

# Now the probability of the balance from one draw.
# I am assuming we have a Normal distribution, thinking of large numbers, but we don't know.
# The problem did not state what distribution we have, and it is possible it could be skewed right.
prob_one_draw <- dnorm(balance, mean=350, sd=58)

# If the question was asking for the probability of drawing one account,
# the probability for such one account being less than 356.6 would be determined as follows:
p_356.6_draw_one_account <- pnorm(356.6, 350, 58)

# Plot the density function pertaining the the daily balances of the accounts
plot(balance, prob_one_draw, 
     type = 'l', 
     main='Average daily balance Normal distribution (shade < $356.60)',
     ylab = 'Normal density',
     xlab = 'Average daily balance per bank account ($USD) \n red line on 356.6',
     ylim = c(0, 0.007))

# From textbook 'Data Analysis and Graphics Using R' by Maindonald and Braun, page 85.
# The area under the shade is the probability that an account will have an average balance less than $356.60
polygon(c(balance[balance < 356.6], 356.6), c(dnorm(balance[balance < 356.6], 350, 58), 0), col='lightblue')
abline(v=356.6, col='red')
text(175, 0.004, paste("One draw, \n p(balance<356.6) = ", round(p_356.6_draw_one_account,2)))
arrows(200, 0.003, 300, 0.002)

# But the problem asks for the mean of sample size 144 accounts
one_144_sample <- rnorm(144, 350, 58)

hist(one_144_sample,
     main='One sample of 144 accounts',
     ylab = 'Frequency',
     xlab = 'Average daily balance from the sample ($USD) \n blue line on mean, red line on 356.6',
     col = 'lightblue')
abline(v=mean(one_144_sample), col='blue')
abline(v=356.6, col='red')


# Now the sample can be drawn many times. 
# The problem ask us to draw a sample of 144 checking accounts
# Let's do this S times for example

# Initialize a vector of S values to store S trials

S <- 30
calculated_mean <- numeric(S)

# And a logical vector to test if the mean is below 356.6
below_356.6 <- logical(S)
for (i in 1:S){
  calculated_mean[i] <- mean(rnorm(144, 350, 58))
  if (calculated_mean[i] < 356.6) {
    below_356.6[i] = TRUE
  } else {
    below_356.6[i] = FALSE
  }
}

sample_mean <- mean(calculated_mean)

sample_sd <- sd(calculated_mean)

sem <- sample_sd / sqrt(S)

# Now determine the probability 
p_356.6_theoretical_sample_means <- pnorm(356.6, sample_mean, sem)

calc_30_prob_below_356.6 <- sum(below_356.6) / S


mean_samples <- pretty(c(340, 360), 200)

# Now the probability of having that amount saved is our height. I will call it height.
# I am assuming we have a Normal distribution, thinking of large numbers, but we don't know.
# The problem did not state what distribution we have, and it is possible it could be skewed right.
prob_samples <- dnorm(mean_samples, mean=sample_mean, sd=sem)

# Plot the density function pertaining the the daily balances of the accounts
plot(mean_samples, prob_samples, 
     type = 'l', 
     main='Means of samples size 144 (shade < $356.60)',
     ylab = 'Normal density',
     xlab = 'Average means of sample size 144 ($USD) \n red line on 356.6')
polygon(c(mean_samples[mean_samples < 356.6], 356.6), c(dnorm(mean_samples[mean_samples < 356.6], sample_mean, sem), 0), col='lightblue')
abline(v=356.6, col='red')
text(344, 0.2, paste("30 samples size 144, \n Calculated p(mean<356.6) \n = ", round(calc_30_prob_below_356.6,2)))
arrows(345, 0.15, 350, 0.1)

### Repeat, but this time use a bigger number like 100
# Initialize a vector of S values to store S trials

S <- 100
calculated_mean <- numeric(S)

# And a logical vector to test if the mean is below 356.6
below_356.6 <- logical(S)
for (i in 1:S){
  calculated_mean[i] <- mean(rnorm(144, 350, 58))
  if (calculated_mean[i] < 356.6) {
    below_356.6[i] = TRUE
  } else {
    below_356.6[i] = FALSE
  }
}

sample_mean <- mean(calculated_mean)

sample_sd <- sd(calculated_mean)

sem <- sample_sd / sqrt(S)

# Now determine the probability 
p_356.6_theoretical_sample_means <- pnorm(356.6, sample_mean, sem)

calc_100_prob_below_356.6 <- sum(below_356.6) / S


mean_samples <- pretty(c(340, 360), 200)

# Now the probability of having that amount saved is our height. I will call it height.
# I am assuming we have a Normal distribution, thinking of large numbers, but we don't know.
# The problem did not state what distribution we have, and it is possible it could be skewed right.
prob_samples <- dnorm(mean_samples, mean=sample_mean, sd=sem)

# Plot the density function pertaining the the daily balances of the accounts
plot(mean_samples, prob_samples, 
     type = 'l', 
     main='Means of samples size 144 (shade < $356.60)',
     ylab = 'Normal density',
     xlab = 'Average means of sample size 144 ($USD) \n red line on 356.6')
polygon(c(mean_samples[mean_samples < 356.6], 356.6), c(dnorm(mean_samples[mean_samples < 356.6], sample_mean, sem), 0), col='lightblue')
abline(v=356.6, col='red')
text(345, 0.3, paste("100 samples size 144, \n Calculated p(mean<356.6) \n = ", round(calc_100_prob_below_356.6,2)))
arrows(347, 0.19, 350, 0.1)



paste('Using a frequentist approach, the probability that the mean of a sample of 144 accounts is less than $ 356.6, based on 30 experiments, is ', round(calc_30_prob_below_356.6,4))
paste('Using a frequentist approach, the probability that the mean of a sample of 144 accounts is less than $ 356.6, based on 100 experiments, is ', round(calc_100_prob_below_356.6,4))
# This will be our horizontal axis in our density plot.

paste('Using a theoretical approach, the probability that the mean of a sample of 144 accounts is less than $ 356.6, based on a mean and SEM from 100 experiments, is ', round(p_356.6_theoretical_sample_means, 6))



```

What is the probability that the sample mean will lie between $340 and $350? 

```{r,eval=TRUE}
#R Code Here
```

